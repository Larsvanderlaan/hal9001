% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/formula_hal9001.R, R/hal.R
\name{formula_hal}
\alias{formula_hal}
\alias{fit_hal_formula}
\alias{fit_hal}
\title{Formula Interface for HAL Fitting Procedure}
\usage{
formula_hal(
  formula,
  data,
  smoothness_orders = NULL,
  num_knots = NULL,
  exclusive_dot = FALSE,
  custom_group = NULL,
  adaptive_smoothing = FALSE,
  ...
)

fit_hal_formula(formula, ...)

fit_hal(
  X,
  Y,
  X_unpenalized = NULL,
  max_degree = ifelse(ncol(X) >= 20, 2, 3),
  smoothness_orders = rep(1, ncol(X)),
  num_knots = sapply(seq_len(max_degree), num_knots_generator, smoothness_orders =
    smoothness_orders, base_num_knots_0 = 500, base_num_knots_1 = 200),
  fit_type = c("glmnet", "lassi"),
  n_folds = 10,
  foldid = NULL,
  use_min = TRUE,
  reduce_basis = NULL,
  family = c("gaussian", "binomial", "poisson", "cox"),
  return_lasso = TRUE,
  return_x_basis = FALSE,
  basis_list = NULL,
  lambda = NULL,
  id = NULL,
  offset = NULL,
  cv_select = TRUE,
  adaptive_smoothing = FALSE,
  prediction_bounds = "default",
  p_reserve = 0.5,
  ...,
  yolo = FALSE
)
}
\arguments{
\item{formula}{A character string specifying the hal9001 model. The format
should be of the form "y ~ h(x) + h(w,x) + h(x,w) + h(x,w,z) ", where "y"
is the outcome and "w,x,y,z" are variables in \code{data}. Each term
represents a main-term/interaction to be included in the model. h(x)
signifies that all one-way/main terms basis functions of the variable x
should be included. h(x,w) specifies that all interaction (two-way) basis
functions between x and w should be included in the model. Similarly,
h(x,w,z) specifies that all interaction (three-way) basis functions between
x,w,z should be included in the model. Note that "y ~ h(x,y,z)" will only
construct three-way basis functions for x, y, z and not the two-way and
one-way basis functions. Additionally, a formula of the form \code{"y ~ ."} will
generate all one-way main term basis functions for variables in
\code{data}. Similarly, \code{"y ~ .^2"} will generate all basis functions up to
degree 2 for all variables in \code{data}. More generally,
\code{"y ~ .^max_degree"} will construct all basis functions up to degree
\code{max_degree}. One can combine all the notions above. For example,
\code{"y ~ h(x,w,z) + ."} and \code{"y ~ h(x,w,z) + .^2"} will generate all one-way
(respectively, up to two-way) basis functions and additionally all the
three-way interaction basis functions between variables w, x, z. One can
also specify monotonicity constraints by replacing the letter \code{h} with \code{d}
(for decreasing) or \code{i} (for increasing), e.g., formulas like
\code{"y ~ i(x) + i(y) + i(x, y)"}, \code{"y ~ d(x) + d(y) + d(x, y)"}, or
\code{"y ~ d(x) + i(y) + h(x, y)"}. The letters h, i, d specify functional
restrictions of each term:
\itemize{
\item \code{h} specifies no constraints on the term,
\item \code{i} specifies the term should be enforced to be monotonely increasing,
\item \code{d} specifies the term should be enforced to be monotonely decreasing.
Ambiguous operations like \code{"y ~ i(x) + ."} will use the first specification
of the term in the formula (generally from left to right). That is,
\code{"y ~ i(x) + ."} is interpreted as \code{"y ~ i(x) + h(z) + h(w)"} while
\code{"y ~ h(x) + i(x)"} is interpreted as \code{"y ~ h(x)"}. NOTE that \code{"."} and
\code{".^max_degree"} have the lowest importance and are evaluated last,
regardless of their location in the formula. As a result, \code{"y ~ . + i(x)"}
will be interpreted as \code{"y ~ i(x) + h(w) + h(z)"}, contrary to the previous
case. Familiar operations such as the \code{:}, \code{*} , \code{-} are also supported:
\item \code{:} is a concatnation operator which maps \code{h(x):h(w)} into \code{h(x,w)} or
\code{h(x):h(w):h(z)} into \code{h(x,w,z)}.
\item \code{*} concatenates and then generates all lower order terms/interactions.
For example, \code{h(x)*h(w)} into \code{h(x) + h(w) + h(x,w)} or \code{h(x)*h(w)*h(z)}
into \code{h(x) + h(w) + h(z) + h(x,w) + h(x,z) + h(z,w) + h(x,w,z)}.
\item \code{-} subtracts/removes the term from the formula. For example, \code{h(x) + h(w) - h(w)} becomes \code{h(x)}.
Note that the above operations are sensitive to the constraint prefix (\code{h},
\code{i}, and \code{d}). For ambigious operations like \code{i(x):h(w)}, the unconstrained
prefix \code{h} will be used unless all prefixes in the term are the same. So,
\code{i(x):h(w)} becomes \code{h(x,w)} and \code{i(x):i(w):d(z)} becomes \code{h(x,w,z)}, and
\code{i(x):i(w)} becomes \code{i(x,w)}. The above logic will be applied recursively
to \code{*} so that \code{i(x) * h(w) * i(z)} is interpreted as \code{i(x) + h(w) + i(z) +  h(x,w) + i(x,z) + h(w,z) + h(x,w,z)}. Another useful operation is the
wildcard \code{.} operator, which when used in a specified term will generate
all valid terms where the value of \code{.} is iterated over the non-outcome
columns in the data matrix. For example, \code{h(x,.)} is \code{h(x,w) + h(x,z)} and
\code{h(.,.)} is \code{h(x,w) + h(x,z) + h(w,z)}, and \verb{h(x,w,.} is \code{h(x,w,z)},
assuming the covariates are only (x, w, z). All operations are compatible
with one another, e.g., \code{h(.)*h(x)}, \code{h(.):h(x)}  and \code{h(x) - h(.)} are
valid and behave as expected.
}}

\item{data}{A \code{data.frame} or named matrix containing the outcome and
covariates specified in the argument \code{formula}.}

\item{smoothness_orders}{An \code{integer} vector of length 1 or length
\code{ncol(X)}. If \code{smoothness_orders} is of length 1, then its values
are recycled to form a vector of length \code{ncol(X)}. Given such a vector
of length \code{ncol(X)}, the ith element specifies the level of smoothness
for the variable corresponding with the ith column in \code{X}. A value of
"0" corresponds with 0th-order splines (piece-wise constant), which assumes
no smoothness or continuity of the true regression function. A value of "1"
corresponds with 1st-order splines (piece-wise linear), which only assumes
continuity of the true regression function. A value of "2" corresponds with
2nd-order splines (piece-wise quadratic and linear terms), which assumes a
single order of differentiability for the true regression function. NOTE:
If \code{smoothness_orders} has length less than \code{ncol(X)}, then its
values are recycled as needed.}

\item{num_knots}{An \code{integer} vector of length 1 or length
\code{max_degree}. If \code{num_knots} is a vector of length 1 then its
values are recycled to produce a vector of length \code{max_degree}. Given
a possibly recycled vector of length \code{max_degree}, \code{num_knots[i]}
specifies the maximum number of knot points used when generating basis
functions of degree i for each covariate. For example, \code{num_knots[1]}
specifies how many knot points to use when generating main-term additive
basis functions. \code{num_knots[2]} specifies how many knot points should
be used when generating each univariate basis function in the 2-tensor
product basis functions. A smaller number of knot points gives rise to a
less smooth function. However, fewer knot points can significantly decrease
runtime. If smoothness_orders is 1 or higher then few knot points (10-30)
are needed to maintain near-optimal performance. When considering setting
\code{smoothness_orders = 0}, too few knot points (<50) can significantly
reduce performance; thus, we recommend specifying a vector of length
\code{max_degree} that decreases exponentially, preventing combinatorial
explosions in the number of higher-degree basis functions generated.
Default: For zero order smoothness (any(\code{smoothness_orders}==0)), the
number of knots by interaction degree d decays as \eqn{500/2^{d-1}}. For
first or higher-order smoothness (all(\code{smoothness_orders}>0)), the
number of knots by interaction degree d decays as \eqn{75/2^{d-1}}. These
defaults ensure that the number of basis functions and thus the complexity
of the optimization problem grows scalably in \code{max_degree}.
\itemize{
\item Some good settings for little to no cost in performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(400, 200, 100).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(100, 75, 50).
}
\item Recommended settings for fairly fast runtime and great performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(200, 100, 50).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(50, 25, 15).
}
\item Recommended settings for fast runtime and good/great performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(100, 50, 25).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(40, 15, 10).
}
\item Recommended settings for very fast runtime and good performance:
\itemize{
\item If smoothness_orders = 0, max_degree = 3, num_knots = c(50, 25, 10).
\item If smoothness_orders = 1+, max_degree = 3, num_knots = c(25, 10, 5).
}
}}

\item{exclusive_dot}{A \code{logical} indicator for whether the ``. and
\code{.^max_degree} arguments in the formula should be treated as exclusive or
inclusive with respect to the variables already specified in the formula.
For example, in `y ~ h(x,w) + .`, should the `.` operator be interpreted as
\itemize{
\item add all one-way basis functions for variables remaining in \code{data}
not yet specified in the formula (i.e., excluding x, w); or,
\item add all one-way basis functions for all variables in the data (including
x, w).
As an example, if \code{exclusive_dot} is set to \code{FALSE}, then \code{y ~  h(x) + .^2} and \code{y ~ .^2} specify the same formula, i.e., generating all
basis functions up to degree 2; however, if \code{exclusive_dot} is set to
\code{TRUE}, then \code{y ~ h(x) + .^2}  encodes a different formula than
\code{y ~ .^2}. Specifically, it means to generate one-way basis functions for
the variable "x" and then all basis functions up to degree 2 for other
variables excluding "x" in \code{data}. As a result, no interactions will
be added for the variable "x".
}}

\item{custom_group}{A named \code{list} with single character names that
represent a group, with its elements being a \code{character} vector of
variable names. This allows the user to specify their own wildcard symbols
(e.g., \code{.}); however, the value of the symbol will be iterated over all
variables specified in the user-supplied group. For example, if one sets
\code{custom_group = list("1" = c("x", "w"), "2" = c("t","r"))}, then the
following formula is mapped from \code{y ~ h(1,2)} to \code{y ~ h(x,t) + h(x,r) + h(w,t) + h(w,r)}, so that all two-way interactions using one variable for
each group are generated. Similarly, \code{y ~ h(1,r)} would be mapped to \code{y ~ h(x,r) + h(w,r)}. Thus, the custom groups operate exactly as \code{.}, except
the possible values are restricted to a specific group.}

\item{adaptive_smoothing}{A \code{logical}, which, if \code{TRUE}, HAL will
perform adaptive smoothing up until the maximum order of smoothness
specified by \code{smoothness_orders}. For example, if
\code{smoothness_orders = 2} and \code{adaptive_smoothing = TRUE}, then HAL
will generate all basis functions of smoothness order 0, 1, and 2, and
data-adaptively select the basis functions to use. WARNING: This can
increase runtime by a factor of 2-3 times depending on value of
\code{smoothness_orders}.}

\item{...}{Other arguments passed to \code{\link[glmnet]{cv.glmnet}}. Please
consult its documentation for a full list of options.}

\item{X}{An input \code{matrix} containing observations and covariates.}

\item{Y}{A \code{numeric} vector of obervations of the outcome variable.}

\item{X_unpenalized}{An input \code{matrix} with the same format as X, that
directly get appended into the design matrix (no basis expansion). No L1
penalization is performed on these covariates.}

\item{max_degree}{The highest order of interaction terms for which the basis
functions ought to be generated. The default (\code{NULL}) corresponds to
generating basis functions for the full dimensionality of the input matrix.}

\item{fit_type}{The specific routine to be called when fitting the Lasso
regression in a cross-validated manner. Choosing the \code{glmnet} option
will result in a call to \code{\link[glmnet]{cv.glmnet}} while \code{lassi}
will produce a (faster) call to a custom Lasso routine.}

\item{n_folds}{Integer for the number of folds to be used when splitting the
data for V-fold cross-validation. This defaults to 10.}

\item{foldid}{An optional \code{numeric} containing values between 1 and
\code{n_folds}, identifying the fold to which each observation is assigned.
If supplied, \code{n_folds} can be missing. In such a case, this vector is
passed directly to \code{\link[glmnet]{cv.glmnet}}.}

\item{use_min}{Specify lambda selected by \code{\link[glmnet]{cv.glmnet}}.
\code{TRUE}, \code{"lambda.min"} is used; otherwise, \code{"lambda.1se"}.}

\item{reduce_basis}{A \code{numeric} value bounded in the open unit interval
indicating the minimum proportion of 1's in a basis function column needed
for the basis function to be included in the procedure to fit the Lasso.
Any basis functions with a lower proportion of 1's than the cutoff will be
removed. This argument defaults to \code{NULL}, in which case all basis
functions are used in the lasso-fitting stage of the HAL algorithm.}

\item{family}{A \code{character} or a \code{\link[stats]{family}} object
(supported by \code{\link[glmnet]{glmnet}}) corresponding to the error/link
family for a generalized linear model. \code{character} options are limited
to "gaussian" for fitting a standard penalized linear model, "binomial" for
penalized logistic regression, "poisson" for penalized Poisson regression,
and "cox" for a penalized proportional hazards model. Note that in all
cases where family is not set to "gaussian", \code{fit_type} is limited to
"glmnet". NOTE: Passing in family objects lead to signficantly slower
performance relative to passing in a character family (if supported). Thus,
for nonparametric (HAL) logistic regression, one should always set
\code{family = "binomial"} and never set \code{family = binomial()}.}

\item{return_lasso}{A \code{logical} indicating whether or not to return
the \code{glmnet} fit of the lasso model.}

\item{return_x_basis}{A \code{logical} indicating whether or not to return
the matrix of (possibly reduced) basis functions used in the HAL lasso fit.}

\item{basis_list}{The full set of basis functions generated from the input
data X (via a call to \code{enumerate_basis}). The dimensionality of this
structure is dim = \eqn{(n * 2^(d-1))}, where n is the number of
observations and d is the number of columns in X.}

\item{lambda}{User-specified array of values of the lambda tuning parameter
of the Lasso L1 regression. If \code{NULL}, \code{\link[glmnet]{cv.glmnet}}
will be used to automatically select a CV-optimal value of this
regularization parameter. If specified, the Lasso L1 regression model will
be fit via \code{glmnet}, returning regularized coefficient values for each
value in the input array.}

\item{id}{a vector of ID values, used to generate cross-validation folds for
cross-validated selection of the regularization parameter lambda.}

\item{offset}{a vector of offset values, used in fitting.}

\item{cv_select}{A \code{logical} specifying whether the array of values
specified should be passed to \code{\link[glmnet]{cv.glmnet}} in order to
pick the optimal value (based on cross-validation) (when set to
\code{TRUE}) or to simply fit along the sequence of values (or single
value) using \code{\link[glmnet]{glmnet}} (when set to \code{FALSE}).}

\item{prediction_bounds}{A vector of size two that provides the lower and
upper bounds for predictions. By default, the predictions are bounded
between \code{min(Y) - sd(Y)} and \code{max(Y) + sd(Y)}. Bounding ensures
that there is no extrapolation and that predictions remain bounded, which
is is necessary for cross-validation selection and/or Super Learning.}

\item{p_reserve}{Sparse matrix pre-allocation proportion. Default value is
0.5.  If a dense HAL design matrix is expected, it would be useful to set
\code{p_reserve} to a higher value.}

\item{yolo}{A \code{logical} indicating whether to print one of a curated
selection of quotes from the HAL9000 computer, from the critically
acclaimed epic science-fiction film "2001: A Space Odyssey" (1968).}
}
\value{
Object of class \code{hal9001}, containing a list of basis
functions, a copy map, coefficients estimated for basis functions, and
timing results (for assessing computational efficiency).
}
\description{
Estimation procedure for HAL, the Highly Adaptive Lasso
}
\details{
The function allows users to specify the functional form/model of
hal9001 similar to in \code{\link[stats]{glm}}. The user can specify which
interactions to include, monotonicity constraints, and smoothness
constraints. The returned \code{formula} object can be fed directly into
\code{fit_hal} and the fit can be run with minimal (no) user input.

The procedure uses a custom C++ implementation to generate a design
matrix consisting of basis functions corresponding to covariates and
interactions of covariates and to remove duplicate columns of indicators.
The Lasso regression is fit to this (usually) very wide matrix using either
a custom implementation (based on \pkg{origami}) or by a call to
\code{\link[glmnet]{cv.glmnet}}.
}
\examples{
n <- 100
p <- 3
x <- xmat <- matrix(rnorm(n * p), n, p)
y_prob <- plogis(3 * sin(x[, 1]) + sin(x[, 2]))
y <- rbinom(n = n, size = 1, prob = y_prob)
hal_fit <- fit_hal(X = x, Y = y, family = "binomial")
preds <- predict(hal_fit, new_data = x)
}
